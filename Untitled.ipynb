{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/menion/anaconda3/envs/ai/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "times_steps = 6\n",
    "element_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\n",
    "digit_to_word_map[0]=\"PAD\"\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    \n",
    "    rand_odd_ints = np.random.choice(range(1,10,2), rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2), rand_seq_len)\n",
    "    \n",
    "    # Padding\n",
    "    if rand_seq_len<6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints, [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints, [0]*(6-rand_seq_len))\n",
    "        \n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "    data = even_sentences+odd_sentences\n",
    "    \n",
    "# Same seq lengths for even, odd sentences\n",
    "seqlens*=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Five One One PAD PAD PAD',\n",
       " 'Three Seven Nine Five One PAD',\n",
       " 'One Three Five One PAD PAD',\n",
       " 'Five Nine Seven PAD PAD PAD',\n",
       " 'Five One Nine Five PAD PAD',\n",
       " 'Five Three Five Seven PAD PAD']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_sentences[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two Two Four PAD PAD PAD',\n",
       " 'Eight Six Eight Six Four PAD',\n",
       " 'Eight Eight Eight Four PAD PAD',\n",
       " 'Eight Two Four PAD PAD PAD',\n",
       " 'Eight Four Six Four PAD PAD',\n",
       " 'Eight Six Two Two PAD PAD']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_sentences[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 4, 3, 4, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlens[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from words to indices\n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "# Inverse map\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "    \n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_batch(batch_size,data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "# seqlens for dynamic calculation\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_dimension], -1.0, 1.0),\n",
    "                             name='embedding')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed, sequence_length = _seqlens, dtype=tf.float32)\n",
    "    \n",
    "weights = {\n",
    "'linear_layer': tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes], mean=0,stddev=.01))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "'linear_layer':tf.Variable(tf.truncated_normal([num_classes], mean=0,stddev=.01))\n",
    "}\n",
    "\n",
    "# Extract the last relevant output and use in a linear layer\n",
    "final_output = tf.matmul(states[1], weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits = final_output, labels = _labels)\n",
    "cross_entropy = tf.reduce_mean(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 0: 21.87500\n",
      "Accuracy at 100: 100.00000\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1), tf.argmax(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size, train_x,train_y, train_seqlens)\n",
    "        sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch, _seqlens:seqlen_batch})\n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n",
    "                                               _labels:y_batch,\n",
    "                                               _seqlens:seqlen_batch})\n",
    "            print(\"Accuracy at %d: %.5f\" % (step, acc))\n",
    "        \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test,seqlen_test = get_sentence_batch(batch_size, test_x,test_y, test_seqlens)\n",
    "        batch_pred,batch_acc = sess.run([tf.argmax(final_output,1), accuracy], \n",
    "                                        feed_dict={_inputs:x_test,\n",
    "                                                   _labels:y_test,\n",
    "                                                   _seqlens:seqlen_test})\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n",
    "        \n",
    "    output_example = sess.run([outputs],feed_dict={_inputs:x_test, _labels:y_test, _seqlens:seqlen_test})\n",
    "    states_example = sess.run([states[1]],feed_dict={_inputs:x_test, _labels:y_test, _seqlens:seqlen_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45338485,  0.22028518,  0.4866209 , -0.39610946, -0.43948761,\n",
       "         0.40012556,  0.4073998 ,  0.50630784,  0.53446972, -0.50361568,\n",
       "        -0.56097639, -0.14107759, -0.37376055, -0.51496172, -0.0420157 ,\n",
       "        -0.40838203, -0.13329203, -0.52536339,  0.50354028,  0.48788941,\n",
       "        -0.30271274, -0.48202905,  0.19885591, -0.35509589,  0.40323624,\n",
       "        -0.40325791, -0.32722124, -0.22728226,  0.47828978, -0.50703698,\n",
       "        -0.48974565, -0.34652722],\n",
       "       [-0.77031815,  0.33554065,  0.75114548, -0.71546733, -0.66875219,\n",
       "         0.73573929,  0.68502128,  0.7549054 ,  0.80797565, -0.74662584,\n",
       "        -0.79945141, -0.19492942, -0.3570503 , -0.78755808, -0.34668693,\n",
       "        -0.76519352, -0.53776652, -0.77256197,  0.83429724,  0.71421516,\n",
       "        -0.72635263, -0.73106301,  0.65684772, -0.75637931,  0.68608886,\n",
       "        -0.42445445, -0.68894297, -0.5665853 ,  0.72541368, -0.73829049,\n",
       "        -0.75690573, -0.6751951 ],\n",
       "       [-0.89282179,  0.69143522,  0.87298918, -0.79984093, -0.8697592 ,\n",
       "         0.86501467,  0.82012713,  0.88763821,  0.88791966, -0.88770103,\n",
       "        -0.91333276, -0.44489715, -0.64310294, -0.8784675 , -0.69635355,\n",
       "        -0.85307622, -0.62101108, -0.90932596,  0.85977584,  0.89789855,\n",
       "        -0.8017863 , -0.86775094,  0.70998091, -0.82303602,  0.83478487,\n",
       "        -0.75179756, -0.78325742, -0.74782038,  0.82297683, -0.85564786,\n",
       "        -0.87516367, -0.82619441],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_example[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of LSTM with multiple layers\n",
    "\n",
    "num_LSTM_layers = 2\n",
    "\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers, state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed, sequence_length = _seqlens, dtype=tf.float32)\n",
    "    \n",
    "# Extract the final state and use in a linear layer\n",
    "final_output = tf.matmul(states[num_LSTM_layers-1][1], weights[\"linear_layer\"]) + biases[\"linear_layer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
